{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09058138-2bd0-4ba2-aa2d-5d19cbd13366",
   "metadata": {},
   "source": [
    "# Generate the dummy data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c5c482-48d4-4505-9551-15f56576686b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faker pandas numpy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "import random\n",
    "from datetime import timedelta\n",
    "\n",
    "fake = Faker('en_IN')\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "num_customers = 60000\n",
    "\n",
    "\n",
    "device_types = ['Mobile', 'Desktop', 'Tablet']\n",
    "genders = ['Male', 'Female', 'Other']\n",
    "membership_types = ['Basic', 'Premium', 'Gold']\n",
    "locations = ['Delhi', 'Mumbai', 'Bangalore', 'Kolkata', 'Chennai', 'Hyderabad', 'Pune', 'Lucknow', 'Jaipur', 'Patna']\n",
    "payment_methods = ['UPI', 'Cash on Delivery', 'Paytm', 'PhonePe', 'Google Pay', 'NetBanking']\n",
    "categories = ['Groceries', 'Electronics', 'Mobile Recharge', 'Medicine', 'Stationery']\n",
    "\n",
    "data = []\n",
    "\n",
    "for _ in range(num_customers):\n",
    "    signup_date = fake.date_between(start_date='-3y', end_date='-1y')\n",
    "    last_login = fake.date_between(start_date=signup_date, end_date='today')\n",
    "    churned = random.choice([0, 1])\n",
    "    churn_date = fake.date_between(start_date=last_login, end_date='today') if churned else None\n",
    "    session_duration = random.randint(60, 900)  # in seconds\n",
    "    pages_visited = random.randint(1, 15)\n",
    "    clicks = random.randint(5, 50)\n",
    "    purchases = random.randint(0, 10)\n",
    "    avg_purchase = round(random.uniform(100, 2000), 2)\n",
    "    total_spent = round(purchases * avg_purchase, 2)\n",
    "    tenure_months = (pd.Timestamp('today') - pd.to_datetime(signup_date)).days // 30\n",
    "    monthly_spend = round(total_spent / tenure_months, 2) if tenure_months > 0 else 0\n",
    "    cart_abandon_rate = round(random.uniform(0.0, 0.8), 2)\n",
    "    feedback_score = random.randint(1, 5)\n",
    "    support_tickets = random.randint(0, 3)\n",
    "    ticket_resolution_time = random.randint(1, 48) if support_tickets > 0 else 0\n",
    "    last_login_gap = (pd.Timestamp('today') - pd.to_datetime(last_login)).days\n",
    "    discount_used = random.choice([0, 1])\n",
    "    payment_method = random.choice(payment_methods)\n",
    "    preferred_category = random.choice(categories)\n",
    "    referral_used = random.choice([0, 1])\n",
    "    referral_count = random.randint(0, 10) if referral_used else 0\n",
    "    device_switch_count = random.randint(0, 5)\n",
    "\n",
    "    data.append({\n",
    "        \"customer_id\": fake.uuid4(),\n",
    "        \"name\": fake.name(),\n",
    "        \"phone_number\": fake.phone_number(),\n",
    "        \"age\": random.randint(18, 65),\n",
    "        \"gender\": random.choice(genders),\n",
    "        \"location\": random.choice(locations),\n",
    "        \"signup_date\": signup_date,\n",
    "        \"last_login_date\": last_login,\n",
    "        \"membership_type\": random.choice(membership_types),\n",
    "        \"device_type\": random.choice(device_types),\n",
    "        \"session_duration\": session_duration,\n",
    "        \"pages_visited\": pages_visited,\n",
    "        \"clicks\": clicks,\n",
    "        \"total_purchases\": purchases,\n",
    "        \"avg_purchase_value_inr\": avg_purchase,\n",
    "        \"total_spent_inr\": total_spent,\n",
    "        \"monthly_spend_inr\": monthly_spend,\n",
    "        \"last_purchase_date\": fake.date_between(start_date=signup_date, end_date='today') if purchases > 0 else None,\n",
    "        \"cart_abandon_rate\": cart_abandon_rate,\n",
    "        \"feedback_score\": feedback_score,\n",
    "        \"support_tickets\": support_tickets,\n",
    "        \"ticket_resolution_time\": ticket_resolution_time,\n",
    "        \"is_active\": bool(1 - churned),\n",
    "        \"churn_date\": churn_date,\n",
    "        \"tenure\": tenure_months,\n",
    "        \"is_churned\": churned,\n",
    "        \"last_login_gap\": last_login_gap,\n",
    "        \"discount_used\": discount_used,\n",
    "        \"payment_method\": payment_method,\n",
    "        \"preferred_category\": preferred_category,\n",
    "        \"referral_used\": referral_used,\n",
    "        \"referral_count\": referral_count,\n",
    "        \"device_switch_count\": device_switch_count\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(\"customer_behavior_analytics_dataset_india.csv\", index=False)\n",
    "\n",
    "print(\"60,000 rows created'customer_behavior_analytics_dataset_india.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cbc74b-463d-42f4-b60d-d7d18bd6dacf",
   "metadata": {},
   "source": [
    "# Send data from csv to kafka as a realtime \n",
    "##### first or all we need to start the zookeper , go to the kafka folder\n",
    "##### use this command to start the zookeper bin\\windows\\zookeeper-server-start.bat config\\zookeeper.properties\n",
    "##### now we need to start the kafka same go to the kafka folder \n",
    "##### use this command bin\\windows\\kafka-server-start.bat config\\server.properties \n",
    "##### now we need to start the producer and use the below code \n",
    "##### make a topic using this command bin\\windows\\kafka-server-start.bat config\\server.properties\n",
    "##### now we need to use the consumer so use the below code \n",
    "##### in the consumer code we can easily store our data to monogo db\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5c67b8-915b-4538-b619-ed45727007fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRODUCER \n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "#put your file \n",
    "df = pd.read_csv('2805_for_model.csv')  \n",
    "\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers='localhost:9092',\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    message = row.to_dict()\n",
    "    producer.send('2805_for_model.csv', value=message)\n",
    "    print(\"Sent:\", message)\n",
    "    time.sleep(0.01)  # we can adjust the speed by increasing or decresing   \n",
    "\n",
    "producer.flush()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e43269a-2e61-4349-a938-e51938301033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSUMER \n",
    "\n",
    "from kafka import KafkaConsumer\n",
    "from pymongo import MongoClient\n",
    "import json\n",
    "\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    '2805_for_model.csv',\n",
    "    bootstrap_servers='localhost:9092',\n",
    "    auto_offset_reset='latest',\n",
    "    value_deserializer=lambda x: json.loads(x.decode('utf-8'))\n",
    ")\n",
    "\n",
    "# if you have your own database and collection then use here \n",
    "mongo_client = MongoClient('mongodb://localhost:27017/')\n",
    "db = mongo_client['customer_db']\n",
    "collection = db['customer_realtime_2805events']\n",
    "\n",
    "\n",
    "print(\"Waiting for messages..\")\n",
    "for msg in consumer:\n",
    "    event = msg.value\n",
    "    \n",
    "    collection.insert_one(event)\n",
    "    print(\"Inserted:\", event)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad992e6b-173a-4fab-b4f8-0ccef1d721b7",
   "metadata": {},
   "source": [
    "# after storing the data into the mongodb we need to train our model for prediction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b9883f-fca1-470b-b978-aef0cdb2ac27",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier  \n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib  \n",
    "\n",
    "\n",
    "# replace with your file \n",
    "df = pd.read_csv(\"1 final_customer_data._Chnagein isactive csv.csv\")  \n",
    "\n",
    "\n",
    "columns_to_drop = [\"customer_id\", \"name\", \"signup_date\", \"last_login_date\", \"last_purchase_date\", \"churn_date\"]\n",
    "df.drop(columns=columns_to_drop, axis=1, inplace=True)\n",
    "\n",
    "categorical_cols = ['gender', 'location', 'membership_type', 'device_type',\n",
    "                    'payment_method', 'preferred_category']\n",
    "\n",
    "df = pd.get_dummies(df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "X = df.drop(\"is_churned\", axis=1)\n",
    "y = df[\"is_churned\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "y_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\n Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"ROC AUC Score:\", roc_auc_score(y_test, y_proba))\n",
    "\n",
    "feature_importances = pd.Series(model.feature_importances_, index=X.columns)\n",
    "top_features = feature_importances.sort_values(ascending=False).head(10)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x=top_features.values, y=top_features.index)\n",
    "plt.title(\"Top 10 Important Features for Churn Prediction\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "joblib.dump(model, \"churn_prediction_model.pkl\")\n",
    "print(\"Model saved as churn_prediction_model.pkl\")\n",
    "\n",
    "joblib.dump(X.columns.tolist(), \"model_features.pkl\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d417b8-0b01-4b5d-81e6-5678ae121319",
   "metadata": {},
   "source": [
    "# now our model is trained and we need to create a api for doing the automation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50ebd06b-b2a3-4a9b-abaf-77596efe1b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we need to create a acc.on the ngrok then we will get the ngrok token and we need to fill those tokens here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "365774ea-b7e6-4049-a6a3-559b87ba8ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyngrok import ngrok\n",
    "\n",
    "\n",
    "# Replace with your actual token\n",
    "ngrok.set_auth_token(\"FILL YOUR TOKENS HERE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ea2a78-18da-4169-b899-a3e520be092e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code will make a api and it will running on a private url \n",
    "from flask import Flask, request, jsonify\n",
    "import joblib\n",
    "import numpy as np\n",
    "import threading\n",
    "from pyngrok import ngrok\n",
    "\n",
    "\n",
    "model = joblib.load(\"churn_prediction_model.pkl\")\n",
    "feature_columns = joblib.load(\"model_features.pkl\")\n",
    "\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return \" Churn Prediction API is Running\"\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    data = request.get_json()\n",
    "    input_features = np.array([[data.get(col, 0) for col in feature_columns]])\n",
    "    churn_probability = float(model.predict_proba(input_features)[0][1])\n",
    "    is_churned = 1 if churn_probability > 0.5 else 0\n",
    "    return jsonify({\n",
    "        \"churn_probability\": round(churn_probability, 4),\n",
    "        \"is_churned\": is_churned\n",
    "    })\n",
    "\n",
    "def run_flask():\n",
    "    app.run(host=\"127.0.0.1\", port=5000, debug=False, use_reloader=False)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    threading.Thread(target=run_flask).start()\n",
    "    public_url = ngrok.connect(5000)\n",
    "    print(f\" Public URL: {public_url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbb65be-1c95-4da2-94b9-956842adf1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "import requests\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import time\n",
    "import os\n",
    "\n",
    "feature_columns = joblib.load(\"model_features.pkl\")\n",
    "\n",
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "db = client[\"customer_db\"]\n",
    "collection = db[\"customer_realtime_2805events\"]\n",
    "\n",
    "# here we need to put the Ngrok url which we generated in the last code \n",
    "NGROK_URL = \"http://xyz123.ngrok.io/predict\"  \n",
    "\n",
    "new_customers = list(collection.find({\"is_predicted\": {\"$ne\": True}}))\n",
    "\n",
    "if not new_customers:\n",
    "    print(\" No new records found in MongoDB.\")\n",
    "    exit()\n",
    "\n",
    "print(f\" Found {len(new_customers)} new records to predict.\\n\")\n",
    "predictions = []\n",
    "\n",
    "for customer in new_customers:\n",
    "    customer_id = customer.get(\"customer_id\", str(customer[\"_id\"]))\n",
    "    payload = {col: customer.get(col, 0) for col in feature_columns}\n",
    "\n",
    "    try:\n",
    "        response = requests.post(NGROK_URL, json=payload, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "\n",
    "            predictions.append({\n",
    "                \"customer_id\": customer_id,\n",
    "                \"churn_probability\": result[\"churn_probability\"],\n",
    "                \"is_churned\": result[\"is_churned\"],\n",
    "                \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                **payload\n",
    "            })\n",
    "\n",
    "            collection.update_one({\"_id\": customer[\"_id\"]}, {\"$set\": {\"is_predicted\": True}})\n",
    "            print(f\" Predicted for {customer_id} → Churn: {result['is_churned']} ({result['churn_probability']})\")\n",
    "        else:\n",
    "            print(f\" Server error for {customer_id}: {response.status_code}\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\" Request failed for {customer_id}: {e}\")\n",
    "\n",
    "    time.sleep(0.5)  \n",
    "\n",
    "# here we are storing it into the drive so it will auto update \n",
    "output_path = \"C:/Users/Dell/OneDrive/apro/churn_predictions.csv\"\n",
    "if predictions:\n",
    "    df = pd.DataFrame(predictions)\n",
    "    file_exists = os.path.exists(output_path)\n",
    "    df.to_csv(output_path, mode='a', header=not file_exists, index=False)\n",
    "    print(f\"\\n💾 All predictions appended to {output_path}\")\n",
    "else:\n",
    "    print(\"\\n No predictions saved. Something went wrong.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cad5caf-ec00-478d-8205-cbfbe3a077e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will create a dashboard for Customer churn insigt which we will use on the daily basis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d488ea-c306-4611-a0b4-0ff2971dbdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now its time for the batch processing so first of all we need to follow the data analysis cycle "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a78728b-e3b9-4e01-bbbf-2b8a0793e811",
   "metadata": {},
   "source": [
    "# we can see our data in MongoDB Compass and as well as in the CLI \n",
    "\n",
    "#### for CLI \n",
    "        open terminal and write mongosh\n",
    "        show dbs\n",
    "        use your_db\n",
    "        show collection \n",
    "        db.your_collection.find().count() or db.your_collection.find()\n",
    "\n",
    "#### for MongoDB Comapss \n",
    "        open it \n",
    "        connect the connection \n",
    "        go to databases\n",
    "        go to collection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abaaf60d-8dcf-47d8-9f4b-5e3d2bdd7d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to use our data and we can do this in the excel becuase we are having only 60 k rows or if we need to use the sql or python that's\n",
    "# totally depend on us "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e83d24b5-7bd7-4a58-a901-246b73f754f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now will make 4 dashboard \n",
    "# 1 customer dashboard\n",
    "# 2 revenue and purchase dashboard\n",
    "# 3 engagment and activity dashboard \n",
    "# 4 retention and churan dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e11e2c-c61c-4661-b312-b5de73d6f809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4519633-f912-4627-838c-e4884b13dda2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac260ef8-aa61-4cce-a137-1a4028ce1801",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481ac993-d74b-493e-bde2-4b85ea7abfd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d8cd6c-0781-4149-b6cd-7d9b58c4b862",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e4f94d-4a9f-4876-b6e1-babb21cc64b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
